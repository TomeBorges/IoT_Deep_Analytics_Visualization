---
title: "Task 3: Evaluate Techniques for Wifi Locationing"
output: html_notebook
---

This notebook contains the answers to Task 3: Evaluate Techniques for Wifi Locationing.

```{r - Initial imports}
library(RMySQL)

library(caret)


library(klaR)
#Avoid loading the MASS package after dplyr
#It may cause a mess with the select function

library(dplyr)
library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(ggfortify)

library(forecast)

library(lubridate)
library(plyr)
library(plotly)


library(gbm)
library(Rcpp)
```


*Retrieve data*

The UJIIndoorLoc database (http://archive.ics.uci.edu/ml/datasets/UJIIndoorLoc) covers three buildings of Universitat Jaume I with 4 or more floors and almost 110.000m2. It can be used for classification, e.g. actual building and floor identification, or regression, e.g. actual longitude and latitude estimation.

The database consists of 19937 training/reference records (trainingData.csv file) and 1111 validation/test records (validationData.csv file).
The 529 attributes contain the WiFi fingerprint, the coordinates where it was taken, and other useful information.
Each WiFi fingerprint can be characterized by the detected Wireless Access Points (WAPs) and the corresponding Received Signal Strength Intensity (RSSI). The intensity values are represented as negative integer values ranging -104dBm (extremely poor signal) to 0dbM. The positive value 100 is used to denote when a WAP was not detected. During the database creation, 520 different WAPs were detected. Thus, the WiFi fingerprint is composed by 520 intensity values.

_Then the coordinates (latitude, longitude, floor) and Building ID are provided as the attributes to be predicted._
The particular space (offices, labs, etc.) and the relative position (inside/outside the space) where the capture was taken have been recorded. Outside means that the capture was taken in front of the door of the space.
Information about who (user), how (android device & version) and when (timestamp) WiFi capture was taken is also recorded.

The datasets contain the following columns:
Attribute 001 (WAP001): Intensity value for WAP001. Negative integer values from -104 to 0 and +100. Positive value 100 used if WAP001 was not detected.
....
Attribute 520 (WAP520): Intensity value for WAP520. Negative integer values from -104 to 0 and +100. Positive Vvalue 100 used if WAP520 was not detected.
Attribute 521 (Longitude): Longitude. Negative real values from -7695.9387549299299000 to -7299.786516730871000
Attribute 522 (Latitude): Latitude. Positive real values from 4864745.7450159714 to 4865017.3646842018.
Attribute 523 (Floor): Altitude in floors inside the building. Integer values from 0 to 4.
Attribute 524 (BuildingID): ID to identify the building. Measures were taken in three different buildings. Categorical integer values from 0 to 2.
Attribute 525 (SpaceID): Internal ID number to identify the Space (office, corridor, classroom) where the capture was taken. Categorical integer values.
Attribute 526 (RelativePosition): Relative position with respect to the Space (1 - Inside, 2 - Outside in Front of the door). Categorical integer values.
Attribute 527 (UserID): User identifier (see database website above). Categorical integer values.
Attribute 528 (PhoneID): Android device identifier (see database website above). Categorical integer values.
Attribute 529 (Timestamp): UNIX Time when the capture was taken. Integer value.


**Retrieve data from memory:**
```{r - Read the datasets from memory}
df_train<-read.csv('./UJIndoorLoc/trainingData.csv')
df_train

df_validation<-read.csv('./UJIndoorLoc/validationData.csv')
df_validation
```

```{r - Check column names}
length(names(df_train %>% select(starts_with("WAP"))))
names(df_train %>% select(!starts_with("WAP")))

length(names(df_validation %>% select(starts_with("WAP"))))
names(df_validation %>% select(!starts_with("WAP")))
```
The dataset contains the expected columns.

*Initial Preprocessing* 

```{r - Confirm datatypes of each column}
table(sapply(df_train, class))
table(sapply(df_validation, class))
```
All columns are originally integer, except for longitude and latitude which are numeric (floats). All columns seem to be correct except for the timestamp. I will convert timestamp into a more readable format: Datetime.
```{r - Convert timestamp into datetime}
df_train$DateTime <- as.POSIXct((df_train$TIMESTAMP), origin="1970-01-01") #Default origin time
df_validation$DateTime <- as.POSIXct((df_validation$TIMESTAMP), origin="1970-01-01") #Default origin time
```


```{r - Are there missing values?}
any(is.na(df_train))
any(is.na(df_validation))
```
No null values, no need to handle them.

*Initial Data exploration of the data:*


```{r - Plot WAP variables in a single graph.}
hist(stack(df_train %>% select(starts_with("WAP")))$values,xlab="WAP power",main="Distribution of WAP intensity")
```
As expected most entries are a null reading (represented as a '100').
Let's filter out these entries to get a better reading:

```{r -  Plot WAP variables in a single graph without null readings}
hist((stack(df_train %>% select(starts_with("WAP"))) %>% filter(values <100))$values,xlab="WAP power",main="Distribution of WAP intensity excluding null readings")
```
As expected we now have a good looking normal distribution. It is slightly skewed to the right.


```{r - Plot of the not related to Wireless Access Points power}
hist(df_train$LONGITUDE,xlab="Longitude",main="Distribution of Longitudes")
hist(df_train$LATITUDE,xlab="Latitude",main="Distribution of Latitude")
hist(df_train$FLOOR,xlab="Floor",main="Distribution of Floors")
hist(df_train$BUILDINGID,xlab="BuildingID",main="Distribution of BuildingIDs")
hist(df_train$SPACEID,xlab="SpaceIDs",main="Distribution of SpaceIDs")
hist(df_train$RELATIVEPOSITION,xlab="Relative Position",main="Distribution of Relative Positions")

hist(df_train$USERID,xlab="User ID",main="Distribution of UserIDs")
hist(df_train$PHONEID,xlab="Phone ID",main="Distribution of Phone IDs")
```


Let's confirm that the distributions are similar in the validation dataset:

```{r -  Plot WAP variables in a single graph without null readings - Validation}
hist((stack(df_validation %>% select(starts_with("WAP"))) %>% filter(values <100))$values,xlab="WAP power",main="Distribution of WAP intensity excluding null readings")
```

```{r - Plot of the not related to Wireless Access Points power - Validation}
hist(df_validation$LONGITUDE,xlab="Longitude",main="Distribution of Longitudes")
hist(df_validation$LATITUDE,xlab="Latitude",main="Distribution of Latitude")
hist(df_validation$FLOOR,xlab="Floor",main="Distribution of Floors")
hist(df_validation$BUILDINGID,xlab="BuildingID",main="Distribution of BuildingIDs")
hist(df_validation$SPACEID,xlab="SpaceIDs",main="Distribution of SpaceIDs")
hist(df_validation$RELATIVEPOSITION,xlab="Relative Position",main="Distribution of Relative Positions")

hist(df_validation$USERID,xlab="User ID",main="Distribution of UserIDs")
hist(df_validation$PHONEID,xlab="Phone ID",main="Distribution of Phone IDs")
```



The distributions are quite similar, the exceptions are in the SPACEID, RELATIVEPOSITION and USERID where the validation dataset is constant throughout. We will confirm this below:

```{r - Non-variant columns in validation df}
table(df_validation$SPACEID)
table(df_validation$RELATIVEPOSITION)
table(df_validation$USERID)
```
Ye, as expected all are null.




*2nd round of Preprocessing the data:*

**Remove variables without any variation**
I noticed that many variables haven't got any variation. It is useless to incorporate these features in any model as they won't add any new information. 
Therefore, I will identify and later remove these variables.

```{r - Identify and remove constant variables}
constant_variables <- names(df_train[, sapply(df_train, function(v) var(v, na.rm=TRUE)==0)])
df_train <- df_train[,sapply(df_train, function(v) var(v, na.rm=TRUE)!=0)]
constant_variables
```

**Sampling**
As mentioned in the Plan of attack, the dataset is too large. Therefore, I will utilize a random subset of the data for the machine learning portion of this project.
```{r - Gather random sample of 2500 entries from train dataset:}
set.seed(42) #Set seed to ensure reproducibility
df_sample_train <- df_train[sample(nrow(df_train), 2500),]
```

```{r - Identify and remove constant variables in sample dataset}
constant_variables <- names(df_sample_train[, sapply(df_sample_train, function(v) var(v, na.rm=TRUE)==0)])
df_sample_train <- df_sample_train[,sapply(df_sample_train, function(v) var(v, na.rm=TRUE)!=0)]
constant_variables
```

*ML Modeling*

Felice recomended to forecast in cascade, firstly classify the easiest parameters (such as building; floor; inside or out...) and then make a regression for location (longitude and latitude).

Let's do that.


The easiest parameter seems to be the relative position (in or out), then the Building ID and finally the floor.
Having these 3 parameters it will hopefully be easy to infer SpaceID through a last classification model.

Experimentally, I found out that the buildingID is the easiest feature to forecast. This seems plausible because this ought to be the feature with less overlap of WAP signaling: a change in the building value imposes the largest physical distance movimentation relatively to a floor or going outside. In other words, a given WAP can send signal to multiple floors or both inside and outside, but due to increased distance, it seems unlikely that it is capable of sending signals to multiple buildings. Additionaly, knowing the building is helpful when forecasting other variables as can be seen here:

```{r - Plot buildingID against floor and relativeposition.}
Building_Floor_graph<-ggplot(data=df_train, aes(x=BUILDINGID, fill = factor(FLOOR))) + geom_bar()
Building_Floor_graph

Building_InsOut_graph<-ggplot(data=df_train, aes(x=BUILDINGID, fill = factor(RELATIVEPOSITION))) + geom_bar()
Building_InsOut_graph

Building_InsOut_graph<-ggplot(data=df_train, aes(x=FLOOR, fill = factor(RELATIVEPOSITION))) + geom_bar()
Building_InsOut_graph

```


**Define Classification Models**

I chose the following models because these are the ones I am most familiarized and from what I understood, provide good results right out of the bat:
Support Vector Machine; Gradient Boosting; Random Forest;
```{r - 10 fold cross validation for 3 models to experiment}
svmfitControl <- trainControl(method = "repeatedcv", 
                              number = 3, # number of folds
                              repeats = 1) #the number of complete sets of folds to compute
gbmfitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
rffitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
```


**Building ID**

First of all let's define the usable features and the target for this model:
```{r - Prepare dataset for training BuildingID}
df_train_BID <- df_sample_train %>% select(starts_with("WAP"),"BUILDINGID")
#Perform factorization on target column:
df_train_BID$BUILDINGID <- as.factor(as.character(df_train_BID$BUILDINGID)) #Reconvert feature into binary column not integer.
```

Because this dataset is slightly unbalanced, I will attempt to train according to the kappa metric instead of accuracy.
(Practically, Cohen’s kappa removes the possibility of the classifier and a random guess agreeing and measures the number of predictions it makes that cannot be explained by a random guess. Furthermore, Cohen’s kappa tries to correct the evaluation bias by taking into account the correct classification by a random guess.)


Let's train each of the 3 models with the training dataset:
## SVM:
```{r - Train Support Vector Machine Classification model for the BUILDING ID}
svmfit_BID <- train(BUILDINGID~., #y/target
                 data = df_train_BID, #X/features
                 method = "svmLinear", #ML algorithm
                 trControl=svmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
                )

svmfit_BID
```
## GBM:
```{r - Train Gradient Boost Machine Classification model for the BUILDING ID}
gbmfit_BID <- train(BUILDINGID~., #y/target
                 data = df_train_BID, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

gbmfit_BID
```
## RF:
```{r - Train Random Forest Classification model for the BUILDING ID}
rffit_BID <- train(BUILDINGID~., #y/target
                 data = df_train_BID, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

rffit_BID
```

As asked, in this part we are supposed to train 3 models, which is what is coded above.

Train function automatically picks the iteration with the largest performance value (the one with best Kappa in this case), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in each model training for the BUILDING ID}
varImp(svmfit_BID)

varImp(gbmfit_BID)

varImp(rffit_BID)
```


A few WAPs are recurrent, such as 087 (top 1 for both SVM and GBM and top 8 for RF), 078 (top 3 for SVM, top 7 for GBM and top 1 for RF)...
This means that we do not need to use all the 500 WAPs, a portion of them will be more than enough. Later I recreate a final model with a smaller amount of features considered.

```{r - Make predictions based on GBM trained model and calculate basic metrics for the BUILDING ID}
BID_pred_SVM <- predict(svmfit_BID, newdata = df_train)
confusionMatrix(data = BID_pred_SVM, 
                reference = factor(df_train$BUILDINGID))

BID_pred_GBM <- predict(gbmfit_BID, newdata = df_train)
confusionMatrix(data = BID_pred_GBM, 
                reference = factor(df_train$BUILDINGID))

BID_pred_RF <- predict(rffit_BID, newdata = df_train)
confusionMatrix(data = BID_pred_RF, 
                reference = factor(df_train$BUILDINGID))
```


These models provide an accuracy of ~99% which is extremely high.
The Kappa or Cohen's Kappa is at ~99% which is quite impressive since this metric is normalized at the baseline of random chance on our dataset.
This dataset has a slightly unbalanced target, therefore the Kappa value is quite different from the accuracy value.
Lastly, it worth noting the P-Value is extremely low, clearly under 0.05, as such, the null hypothesis (that the BUILDING ID is randomly created and is in no way forecastable through the other features in the dataset) can be rejected.

Now let's compare the accuracy and Kappa from the three models:
```{r - Compare BUILDINGID forecasting results in the training dataset:}
results <- resamples(list(SVM_Model=svmfit_BID, GBM_Model=gbmfit_BID, RF_Model=rffit_BID))
summary(results)
bwplot(results)
```
Even though it is pretty close, it seems that the best model is the Gradient Boosting Machine. Therefore we will use the predicted outcome from this model.
But first I will recreate the model with less features to make it more robust:

```{r - Recreate the best model with less features for the BUILDING ID:}

col_index <- varImp(gbmfit_BID)$importance %>% 
  mutate(names=row.names(.)) %>%
  arrange(-Overall)

imp_names <- col_index$names[1:250]

df_train_BID_final <- cbind(df_train_BID[,imp_names],
                            BUILDINGID = df_train_BID$BUILDINGID)

final_fit_BID <- train(BUILDINGID~., #y/target
                 data = df_train_BID_final, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

results <- resamples(list(SVM_Model=svmfit_BID, GBM_Model=gbmfit_BID, RF_Model=rffit_BID, BestModel = final_fit_BID))
summary(results)
bwplot(results)

BID_pred_final <- predict(final_fit_BID, newdata = df_train)
confusionMatrix(data = BID_pred_final, 
                reference = factor(df_train$BUILDINGID))
```
We have managed to improve the results of the GBM model through reducing the entropy of the model (through reducing quantity of variables)

```{r - Create prediction for validation dataset with outcome from best model:}

Validation_BID_pred <- predict(final_fit_BID, newdata = df_validation)
Validation_BID_pred

```

```{r - Bind WAP values with predicted values to create dataset for next level of cascade}
Step1_df_validation <- cbind(df_validation %>% select(starts_with("WAP")), BUILDINGID_pred =Validation_BID_pred)
Step1_df_validation
```


**Floor**

First of all let's define the usable features and the target for this model. We now have predicted the Building ID, therefore, this feature is now "unlocked" and is usable to forecast the Floor feature:
```{r - Prepare dataset for training this model}
df_train_F <- df_sample_train %>% select(starts_with("WAP"),"BUILDINGID","FLOOR")
#Perform factorization on target column:
df_train_F$FLOOR <- as.factor(as.character(df_train_F$FLOOR)) #Reconvert brand feature binary column not integer.
#df_train_F$BUILDINGID <- as.factor(as.integer(df_train_F$BUILDINGID)) #Reconvert feature into binary column not integer.
```

Because this dataset is slightly unbalanced, I will attempt to train according to the kappa metric instead of accuracy.
(Practically, Cohen’s kappa removes the possibility of the classifier and a random guess agreeing and measures the number of predictions it makes that cannot be explained by a random guess. Furthermore, Cohen’s kappa tries to correct the evaluation bias by taking into account the correct classification by a random guess.)



Let's train each of the 3 models with the training dataset:

```{r - Train Support Vector Machine Classification model for Floor:}
svmfit_F <- train(FLOOR~., #y/target
                 data = df_train_F, #X/features
                 method = "svmLinear", #ML algorithm
                 trControl=svmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
                )

svmfit_F
```

```{r - Train Gradient Boost Machine Classification model for Floor:}
gbmfit_F <- train(FLOOR~., #y/target
                 data = df_train_F, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

gbmfit_F
```

```{r - Train Random Forest Classification model for Floor:}
rffit_F <- train(FLOOR~., #y/target
                 data = df_train_F, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)



rffit_F
```

As asked, in this part we are supposed to train 3 models, which is what is coded above.
Train function automatically picks the iteration with the largest performance value (the one with best Kappa in this case), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in each model training}
varImp(svmfit_F)

varImp(gbmfit_F)

varImp(rffit_F)
```


A few WAPs are recurrent, such as 087, 481, 145...
Once again, this means that we do not need to use all the 500 WAPs, a portion of them will be more than enough. Later I recreate a final model with a smaller amount of features considered.

```{r - Make predictions based on GBM trained model and calculate basic metrics for the Floor variable}
F_pred_SVM <- predict(svmfit_F, newdata = df_train)
confusionMatrix(data = F_pred_SVM, 
                reference = factor(df_train$FLOOR))

F_pred_GBM <- predict(gbmfit_F, newdata = df_train)
confusionMatrix(data = F_pred_GBM, 
                reference = factor(df_train$FLOOR))

F_pred_RF <- predict(rffit_F, newdata = df_train)
confusionMatrix(data = F_pred_RF, 
                reference = factor(df_train$FLOOR))
```

The best model, Random Forest, provides an accuracy of ~95% which is extremely high.
The Kappa or Cohen's Kappa from this model is is at ~94% which is quite impressive since this metric is normalized at the baseline of random chance on our dataset.
This dataset has a slightly unbalanced target, therefore the Kappa value is quite different from the accuracy value.

Now let's visualize graphically the accuracy and Kappa from the three models:

```{r - Compare FLOOR forecasting results in the training dataset:}
results <- resamples(list(SVM_Model=svmfit_F, GBM_Model=gbmfit_F, RF_Model=rffit_F))
summary(results)
bwplot(results)
```


Even though it is pretty close, it seems that the best model is the Random Forest Therefore we will use the predicted outcome from this model.
But first I will recreate the model with less features to make it more robust:

```{r - Recreate the best model (For forecasting Floor) with less features:}

col_index <- varImp(rffit_F)$importance %>% 
  mutate(names=row.names(.)) %>%
  arrange(-Overall)

imp_names <- col_index$names[1:250]

df_train_F_final <- cbind(df_train_F[,imp_names],
                            FLOOR = df_train_F$FLOOR)

final_fit_F <- train(FLOOR~., #y/target
                 data = df_train_F_final, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

results <- resamples(list(SVM_Model=svmfit_F, GBM_Model=gbmfit_F, RF_Model=rffit_F, BestModel = final_fit_F))
summary(results)
bwplot(results)

varImp(final_fit_F)[1:250]

F_pred_final <- predict(final_fit_F, newdata = df_train)
confusionMatrix(data = F_pred_final, 
                reference = factor(df_train$FLOOR))
```

We have managed to improve the results of the RF model through reducing the entropy of the model (through reducing quantity of variables)

```{r - Create prediction for validation dataset with outcome from best model:}

Validation_F_pred <- predict(final_fit_F, newdata = df_validation)
Validation_F_pred

```

```{r - Bind WAP + BuildingID values with predicted values to create dataset for next level of cascade}
Step2_df_validation <- cbind(Step1_df_validation, FLOOR_pred = Validation_F_pred)
Step2_df_validation
```

**Relative Position**

First of all let's define the usable features and the target for this model. We now have predicted the Building ID, and Floor therefore, these features are now "unlocked" and usable:
```{r - Prepare dataset for training RelativePosition}
df_train_RP <- df_sample_train %>% select(starts_with("WAP"),"FLOOR","BUILDINGID","RELATIVEPOSITION")

#df_train_RP$BUILDINGID <- as.factor(as.character(df_train_RP$BUILDINGID)) #Reconvert feature into binary column not integer.
#df_train_F$FLOOR <- as.factor(as.character(df_train_F$FLOOR)) #Reconvert brand feature binary column not integer.
#Perform factorization on target column:
df_train_RP$RELATIVEPOSITION <- as.factor(as.character(df_train_RP$RELATIVEPOSITION)) #Reconvert feature into binary column not integer.
```

Because this dataset is slightly unbalanced, I will attempt to train according to the kappa metric instead of accuracy.
(Practically, Cohen’s kappa removes the possibility of the classifier and a random guess agreeing and measures the number of predictions it makes that cannot be explained by a random guess. Furthermore, Cohen’s kappa tries to correct the evaluation bias by taking into account the correct classification by a random guess.)


Let's train each of the 3 models with the training dataset:

```{r - Train Support Vector Machine classification model for the RelativePosition feature}
svmfit_RP <- train(RELATIVEPOSITION~., #y/target
                 data = df_train_RP, #X/features
                 method = "svmLinear", #ML algorithm
                 trControl=svmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
                )

svmfit_RP
```

```{r - Train Gradient Boost Machine Classification model for the RelativePosition feature}
gbmfit_RP <- train(RELATIVEPOSITION~., #y/target
                 data = df_train_RP, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

gbmfit_RP
```

```{r - Train Random Forest Classification model for the RelativePosition feature}
rffit_RP <- train(RELATIVEPOSITION~., #y/target
                 data = df_train_RP, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

rffit_RP
```

As asked, in this part we are supposed to train 3 models, which is what is coded above.

Train function automatically picks the iteration with the largest performance value (the one with best Kappa in this case), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in each model training}
varImp(svmfit_RP)

varImp(gbmfit_RP)

varImp(rffit_RP)
```



In order to predict the Relative Position, the BUILDINGID and FLOOR features are the most relevant. Therefore there is a clear upside to forecasting this feature after the other two.
Also, a few WAPs are recurrent, such as 338 or 456.
This means that we do not need to use all the 500 WAPs, a portion of them will be more than enough. Later I recreate a final model with a smaller amount of features considered.

```{r - Make predictions based on GBM trained model and calculate basic metrics (with PostResample)}
RP_pred_SVM <- predict(svmfit_RP, newdata = df_train)
confusionMatrix(data = RP_pred_SVM, 
                reference = factor(df_train$RELATIVEPOSITION))

RP_pred_GBM <- predict(gbmfit_RP, newdata = df_train)
confusionMatrix(data = RP_pred_GBM, 
                reference = factor(df_train$RELATIVEPOSITION))

RP_pred_RF <- predict(rffit_RP, newdata = df_train)
confusionMatrix(data = RP_pred_RF, 
                reference = factor(df_train$RELATIVEPOSITION))
```


The best model, Random Forest, provides an accuracy of ~90% which is pretty high.
The Kappa or Cohen's Kappa from this model is is at ~58% which is good since this metric is normalized at the baseline of random chance on our dataset.
This dataset has a slightly unbalanced target, therefore the Kappa value is quite different from the accuracy value.

Now let's visualize graphically the accuracy and Kappa from the three models:

```{r - Compare RELATIVEPOSITION forecasting results in the training dataset:}
results <- resamples(list(SVM_Model=svmfit_RP, GBM_Model=gbmfit_RP, RF_Model=rffit_RP))
summary(results)
bwplot(results)
```


Even though it is pretty close, it seems that, once again, the best model is the Random Forest. Therefore we will use the predicted outcome from this model.
But first I will recreate the model with less features to make it more robust:

```{r - Recreate the best model with less features:}

col_index <- varImp(rffit_RP)$importance %>% 
  mutate(names=row.names(.)) %>%
  arrange(-Overall)

imp_names <- col_index$names[1:300]

df_train_RP_final <- cbind(df_train_RP[,imp_names],
                            RELATIVEPOSITION = df_train_RP$RELATIVEPOSITION)

final_fit_RP <- train(RELATIVEPOSITION~., #y/target
                 data = df_train_RP_final, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

results <- resamples(list(SVM_Model=svmfit_RP, GBM_Model=gbmfit_RP, RF_Model=rffit_RP, BestModel = final_fit_RP))
summary(results)
bwplot(results)

varImp(final_fit_RP)[1:250]

RP_pred_final <- predict(final_fit_RP, newdata = df_train)
confusionMatrix(data = RP_pred_final, 
                reference = factor(df_train$RELATIVEPOSITION))
```

We haven't managed to improve the results of the RF model through reducing the entropy of the model (through reducing quantity of variables). However, this models is more robust in the sense that requires less variables and less entropy.


```{r - Create prediction for validation dataset with outcome from best model:}

Validation_RP_pred <- predict(final_fit_RP, newdata = df_validation)
Validation_RP_pred

```

```{r - Bind WAP values + BuildingID + Floor with predicted values to create dataset for next level of cascade}
Step3_df_validation <- cbind(Step2_df_validation, RELATIVEPOSITION_pred = Validation_RP_pred)
Step3_df_validation
```





I have now forecasted the categorical/nominal features. Now it is time to move on to the numerical ones: Longitude & Latitude.

**Define Regression Models**


I will recreate the strategy employed in the classification models, and develop one model for each of the features to be forecasted.


**LONGITUDE**

First of all let's define the usable features and the target for this model. We now have predicted the Building ID, Floor and RelativePosition, i.e., all categorical features, therefore, these are now "unlocked" and usable:

```{r - Prepare dataset for training LONGITUDE}
df_train_LONG <- df_sample_train %>% select(starts_with("WAP"),"FLOOR","BUILDINGID","RELATIVEPOSITION","LONGITUDE")
```

Because this dataset is slightly unbalanced, I will attempt to train according to the kappa metric instead of accuracy.
(Practically, Cohen’s kappa removes the possibility of the classifier and a random guess agreeing and measures the number of predictions it makes that cannot be explained by a random guess. Furthermore, Cohen’s kappa tries to correct the evaluation bias by taking into account the correct classification by a random guess.)

In order to choose the optimization metric:
-Both RMSE and R2 quantify how well a regression model fits a dataset.
-The RMSE tells us how well a regression model can predict the value of the response variable in absolute terms while R2 tells us how well a model can predict the value of the response variable in percentage terms.
*-R2 does not detect overfitting and is not the best for nonlinear relations*

Therefore, I will use the RMSE metric as the point of this regression is to forecast the absolute Volume as accurately as possible (also the results were better utilizing this metric).


Let's train each of the 3 models with the training dataset:

```{r - Train Support Vector Machine classification model for the RelativePosition feature}
svmfit_LONG <- train(LONGITUDE~., #y/target
                 data = df_train_LONG, #X/features
                 method = "svmLinear", #ML algorithm
                 trControl=svmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
                )

svmfit_LONG
```

```{r - Train Gradient Boost Machine Classification model for the LONGITUDE feature}
gbmfit_LONG <- train(LONGITUDE~., #y/target
                 data = df_train_LONG, #X/features
                 metric = 'RMSE', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

gbmfit_LONG
```

```{r - Train Random Forest Classification model for the LONGITUDE feature}
rffit_LONG <- train(LONGITUDE~., #y/target
                 data = df_train_LONG, #X/features
                 metric = 'RMSE', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

rffit_LONG
```

As asked, in this part we are supposed to train 3 models, which is what is coded above.

Train function automatically picks the iteration with the largest performance value (the one with best Kappa in this case), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in each model training}
varImp(svmfit_LONG)

varImp(gbmfit_LONG)

varImp(rffit_LONG)
```



In order to predict the LONGITUDE, the BUILDINGID and FLOOR features are the most relevant. Therefore there is a clear upside to forecasting this feature after the other two.
Also, a few WAPs are recurrent, such as 338 or 456.
This means that we do not need to use all the 500 WAPs, a portion of them will be more than enough. Later I recreate a final model with a smaller amount of features considered.

```{r - Make predictions based on GBM trained model and calculate basic metrics (with PostResample)}
LONG_pred_SVM <- predict(svmfit_LONG, newdata = df_train)
postResample(LONG_pred_SVM, df_train$LONGITUDE)

LONG_pred_GBM <- predict(gbmfit_LONG, newdata = df_train)
postResample(LONG_pred_GBM, df_train$LONGITUDE)

LONG_pred_RF <- predict(rffit_LONG, newdata = df_train)
postResample(LONG_pred_RF, df_train$LONGITUDE)
```


The best model, Random Forest, provides an accuracy of ~90% which is pretty high.
The Kappa or Cohen's Kappa from this model is is at ~58% which is good since this metric is normalized at the baseline of random chance on our dataset.
This dataset has a slightly unbalanced target, therefore the Kappa value is quite different from the accuracy value.

Now let's visualize graphically the accuracy and Kappa from the three models:

```{r - Compare RELATIVEPOSITION forecasting results in the training dataset:}
results <- resamples(list(SVM_Model=svmfit_LONG, GBM_Model=gbmfit_LONG, RF_Model=rffit_LONG))
summary(results)
bwplot(results)
```


The model with the smallest error is the Random Forest. Therefore we will use the predicted outcome from this model.
But first I will recreate the model with less features to make it more robust:

```{r - Recreate the best model with less features to forecast LONGITUDE:}

col_index <- varImp(rffit_LONG)$importance %>% 
  mutate(names=row.names(.)) %>%
  arrange(-Overall)
imp_names <- col_index$names[1:250]

df_train_LONG_final <- cbind(df_train_LONG[,imp_names],
                            LONGITUDE = df_train_LONG$LONGITUDE)



final_fit_LONG <- train(LONGITUDE~., #y/target
                 data = df_train_LONG_final, #X/features
                 metric = 'RMSE', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

results <- resamples(list(SVM_Model=svmfit_LONG, GBM_Model=gbmfit_LONG, RF_Model=rffit_LONG, BestModel = final_fit_LONG))
summary(results)
bwplot(results)

varImp(final_fit_LONG)[1:250]

LONG_pred_final <- predict(final_fit_LONG, newdata = df_train)
postResample(LONG_pred_final, df_train$LONGITUDE)
```



```{r - Create prediction for validation dataset with outcome from best model:}

Validation_LONG_pred <- predict(final_fit_LONG, newdata = df_validation)


```

```{r - Bind WAP values + BuildingID + Floor with predicted values to create dataset for next level of cascade}
Step4_df_validation <- cbind(Step3_df_validation, LONGITUDE_pred = Validation_LONG_pred)
Step4_df_validation
```



**LATITUDE**

First of all let's define the usable features and the target for this model. We now have predicted the Building ID, Floor and RelativePosition, i.e., all categorical features, therefore, these are now "unlocked" and usable:

```{r - Prepare dataset for training LONGITUDE}
df_train_LAT <- df_sample_train %>% select(starts_with("WAP"),"FLOOR","BUILDINGID","RELATIVEPOSITION","LATITUDE")
```

Because this dataset is slightly unbalanced, I will attempt to train according to the kappa metric instead of accuracy.
(Practically, Cohen’s kappa removes the possibility of the classifier and a random guess agreeing and measures the number of predictions it makes that cannot be explained by a random guess. Furthermore, Cohen’s kappa tries to correct the evaluation bias by taking into account the correct classification by a random guess.)

In order to choose the optimization metric:
-Both RMSE and R2 quantify how well a regression model fits a dataset.
-The RMSE tells us how well a regression model can predict the value of the response variable in absolute terms while R2 tells us how well a model can predict the value of the response variable in percentage terms.
*-R2 does not detect overfitting and is not the best for nonlinear relations*

Therefore, I will use the RMSE metric as the point of this regression is to forecast the absolute Volume as accurately as possible (also the results were better utilizing this metric).


Let's train each of the 3 models with the training dataset:

```{r - Train Support Vector Machine classification model for the RelativePosition feature}
svmfit_LAT <- train(LATITUDE~., #y/target
                 data = df_train_LAT, #X/features
                 method = "svmLinear", #ML algorithm
                 trControl=svmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
                )

svmfit_LAT
```

```{r - Train Gradient Boost Machine Classification model for the LONGITUDE feature}
gbmfit_LAT <- train(LATITUDE~., #y/target
                 data = df_train_LAT, #X/features
                 metric = 'RMSE', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

gbmfit_LAT
```

```{r - Train Random Forest Classification model for the LONGITUDE feature}
rffit_LAT <- train(LATITUDE~., #y/target
                 data = df_train_LAT, #X/features
                 metric = 'RMSE', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

rffit_LAT
```

As asked, in this part we are supposed to train 3 models, which is what is coded above.

Train function automatically picks the iteration with the largest performance value (the one with best Kappa in this case), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in each model training}
varImp(svmfit_LAT)

varImp(gbmfit_LAT)

varImp(rffit_LAT)
```



In order to predict the LATITUDE, the BUILDINGID and FLOOR features are the most relevant. Therefore there is a clear upside to forecasting this feature after the other two.
Also, a few WAPs are recurrent, such as 338 or 456.
This means that we do not need to use all the 500 WAPs, a portion of them will be more than enough. Later I recreate a final model with a smaller amount of features considered.

```{r - Make predictions based on GBM trained model and calculate basic metrics (with PostResample)}
LAT_pred_SVM <- predict(svmfit_LAT, newdata = df_train)
postResample(LAT_pred_SVM, df_train$LATITUDE)

LAT_pred_GBM <- predict(gbmfit_LAT, newdata = df_train)
postResample(LAT_pred_GBM, df_train$LATITUDE)

LAT_pred_RF <- predict(rffit_LAT, newdata = df_train)
postResample(LAT_pred_RF, df_train$LATITUDE)
```


The best model, Random Forest, provides an accuracy of ~90% which is pretty high.
The Kappa or Cohen's Kappa from this model is is at ~58% which is good since this metric is normalized at the baseline of random chance on our dataset.
This dataset has a slightly unbalanced target, therefore the Kappa value is quite different from the accuracy value.

Now let's visualize graphically the accuracy and Kappa from the three models:

```{r - Compare RELATIVEPOSITION forecasting results in the training dataset:}
results <- resamples(list(SVM_Model=svmfit_LAT, GBM_Model=gbmfit_LAT, RF_Model=rffit_LAT))
summary(results)
bwplot(results)
```


The model with the smallest error is the Random Forest. Therefore we will use the predicted outcome from this model.
But first I will recreate the model with less features to make it more robust:

```{r - Recreate the best model with less features to forecast LONGITUDE:}

col_index <- varImp(rffit_LAT)$importance %>% 
  mutate(names=row.names(.)) %>%
  arrange(-Overall)
imp_names <- col_index$names[1:250]

df_train_LAT_final <- cbind(df_train_LAT[,imp_names],
                            LATITUDE = df_train_LAT$LATITUDE)



final_fit_LAT <- train(LATITUDE~., #y/target
                 data = df_train_LAT_final, #X/features
                 metric = 'RMSE', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

results <- resamples(list(SVM_Model=svmfit_LAT, GBM_Model=gbmfit_LAT, RF_Model=rffit_LAT, BestModel = final_fit_LAT))
summary(results)
bwplot(results)

varImp(final_fit_LAT)[1:250]

LAT_pred_final <- predict(final_fit_LAT, newdata = df_train)
postResample(LAT_pred_final, df_train$LATITUDE)
```



```{r - Create prediction for validation dataset with outcome from best model:}

Validation_LAT_pred <- predict(final_fit_LAT, newdata = df_validation)


```

```{r - Bind WAP values + BuildingID + Floor with predicted values to create dataset for next level of cascade}
Step5_df_validation <- cbind(Step4_df_validation, LATITUDE_pred = Validation_LAT_pred)
Step5_df_validation
```

*Interpret results now*

```{r - Longitude and Latitude distributions:}
hist(df_validation$BUILDINGID,xlab="BuildingID",main="Actual Distribution of BuildingID")
hist(df_validation$FLOOR,xlab="Floor",main="Actual Distribution of Floor")
hist(df_validation$RELATIVEPOSITION,xlab="RelativePosition",main="Actual Distribution of RelativePosition")

hist(as.numeric(Step5_df_validation$BUILDINGID_pred),xlab="BuildingID",main="Predicted Distribution of BuildingID")
hist(as.numeric(Step5_df_validation$FLOOR_pred),xlab="Floor",main="Predicted Distribution of Floor")
hist(as.numeric(Step5_df_validation$RELATIVEPOSITION_pred),xlab="RelativePosition",main="Predicted Distribution of RelativePosition")
```

```{r - Longitude and Latitude distributions:}
hist(df_validation$LONGITUDE,xlab="Longitude",main="Actual Distribution of Longitudes")
hist(df_validation$LATITUDE,xlab="Latitude",main="Actual Distribution of Latitude")

hist(Step5_df_validation$LONGITUDE_pred,xlab="Longitude",main="Predicted Distribution of Longitudes")
hist(Step5_df_validation$LATITUDE_pred,xlab="Latitude",main="Predicted Distribution of Latitude")
```

The distributions are quite similar between the actual values and the forecasted, which is a good sign.


```{r}
results <- resamples(list(BuildingID_Model=final_fit_BID, 
                          Floor_Model = final_fit_F,
                          RelativePosition_Model = final_fit_RP))
summary(results)
bwplot(results)
```
```{r}
results <- resamples(list(Latitude_Model = final_fit_LAT,
                          Longitude_Model = final_fit_LONG))
summary(results)
bwplot(results)
```

After research I found that the coordinates from this dataset are in EPSG:3857, WGS 84 / Pseudo-Mercator format. Using the following library I can find distances in predictions from two different points.

```{r - Calculate distances from predicted and actual values:}
library(sf)

line = st_sfc(st_linestring(rbind(c(0,0),
                                  c(1000,0))), crs = 3857)
st_length(line)

```
After reading about this type of coordinate format (such as: https://gis.stackexchange.com/questions/242545/how-can-epsg3857-be-in-meters) and after running the chunk above and concluding that the euclidian distance between the point (0,0) and (1000,0) in EPSG:3857 coordinates is in fact 1000m, I can conclude that the distance between two points, or that the errors of the longitude and latitude models using this coordinate system can be considered in meters.

As such, this system with cascade models managed to gather a Median average error of 5.8 meters in latitude and 6.7 meters in longitude which results in a vectorial error of under 9 meters which is comparable to the accuracy of the GPS system (sources vary between 4 and 15 meters) in an outside environment. Because the users are inside a building, the GPS accuracy would worsen, therefore this model has the potential of being a better option for locating users.


```{r - Print raw predictions of final position features}
#Step5_df_validation%>% select(ends_with("pred"))
write.csv(Step5_df_validation%>% select("BUILDINGID_pred",
                                        "FLOOR_pred",
                                        "RELATIVEPOSITION_pred",
                                        "LONGITUDE_pred",
                                        "LATITUDE_pred")
          ,"RawPredictions.csv", row.names = TRUE)
```

