---
title: "Task 3: Evaluate Techniques for Wifi Locationing"
output: html_notebook
---

This notebook contains the answers to Task 3: Evaluate Techniques for Wifi Locationing.

```{r - Initial imports}
library(RMySQL)

library(caret)


library(klaR)
#Avoid loading the MASS package after dplyr
#It may cause a mess with the select function

library(dplyr)
library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(ggfortify)

library(forecast)

library(lubridate)
library(plyr)
library(plotly)


library(gbm)
library(Rcpp)
```


*Retrieve data*

The UJIIndoorLoc database (http://archive.ics.uci.edu/ml/datasets/UJIIndoorLoc) covers three buildings of Universitat Jaume I with 4 or more floors and almost 110.000m2. It can be used for classification, e.g. actual building and floor identification, or regression, e.g. actual longitude and latitude estimation.

The database consists of 19937 training/reference records (trainingData.csv file) and 1111 validation/test records (validationData.csv file).
The 529 attributes contain the WiFi fingerprint, the coordinates where it was taken, and other useful information.
Each WiFi fingerprint can be characterized by the detected Wireless Access Points (WAPs) and the corresponding Received Signal Strength Intensity (RSSI). The intensity values are represented as negative integer values ranging -104dBm (extremely poor signal) to 0dbM. The positive value 100 is used to denote when a WAP was not detected. During the database creation, 520 different WAPs were detected. Thus, the WiFi fingerprint is composed by 520 intensity values.

_Then the coordinates (latitude, longitude, floor) and Building ID are provided as the attributes to be predicted._
The particular space (offices, labs, etc.) and the relative position (inside/outside the space) where the capture was taken have been recorded. Outside means that the capture was taken in front of the door of the space.
Information about who (user), how (android device & version) and when (timestamp) WiFi capture was taken is also recorded.

The datasets contain the following columns:
Attribute 001 (WAP001): Intensity value for WAP001. Negative integer values from -104 to 0 and +100. Positive value 100 used if WAP001 was not detected.
....
Attribute 520 (WAP520): Intensity value for WAP520. Negative integer values from -104 to 0 and +100. Positive Vvalue 100 used if WAP520 was not detected.
Attribute 521 (Longitude): Longitude. Negative real values from -7695.9387549299299000 to -7299.786516730871000
Attribute 522 (Latitude): Latitude. Positive real values from 4864745.7450159714 to 4865017.3646842018.
Attribute 523 (Floor): Altitude in floors inside the building. Integer values from 0 to 4.
Attribute 524 (BuildingID): ID to identify the building. Measures were taken in three different buildings. Categorical integer values from 0 to 2.
Attribute 525 (SpaceID): Internal ID number to identify the Space (office, corridor, classroom) where the capture was taken. Categorical integer values.
Attribute 526 (RelativePosition): Relative position with respect to the Space (1 - Inside, 2 - Outside in Front of the door). Categorical integer values.
Attribute 527 (UserID): User identifier (see database website above). Categorical integer values.
Attribute 528 (PhoneID): Android device identifier (see database website above). Categorical integer values.
Attribute 529 (Timestamp): UNIX Time when the capture was taken. Integer value.


**Retrieve data from memory:**
```{r - Read the datasets from memory}
df_train<-read.csv('./UJIndoorLoc/trainingData.csv')
df_train

df_validation<-read.csv('./UJIndoorLoc/validationData.csv')
df_validation
```

```{r - Check column names}
length(names(df_train %>% select(starts_with("WAP"))))
names(df_train %>% select(!starts_with("WAP")))

length(names(df_validation %>% select(starts_with("WAP"))))
names(df_validation %>% select(!starts_with("WAP")))
```
The dataset contains the expected columns.

*Initial Preprocessing* 

```{r - Confirm datatypes of each column}
table(sapply(df_train, class))
table(sapply(df_validation, class))
```
All columns are originally integer, except for longitude and latitude which are numeric (floats). All columns seem to be correct except for the timestamp. I will convert timestamp into a more readable format: Datetime.
```{r - Convert timestamp into datetime}
df_train$DateTime <- as.POSIXct((df_train$TIMESTAMP), origin="1970-01-01") #Default origin time
df_validation$DateTime <- as.POSIXct((df_validation$TIMESTAMP), origin="1970-01-01") #Default origin time
```


```{r - Are there missing values?}
any(is.na(df_train))
any(is.na(df_validation))
```
No null values, no need to handle them.

*Initial Data exploration of the data:*


```{r - Plot WAP variables in a single graph.}
hist(stack(df_train %>% select(starts_with("WAP")))$values,xlab="WAP power",main="Distribution of WAP intensity")
```
As expected most entries are a null reading (represented as a '100').
Let's filter out these entries to get a better reading:

```{r -  Plot WAP variables in a single graph without null readings}
hist((stack(df_train %>% select(starts_with("WAP"))) %>% filter(values <100))$values,xlab="WAP power",main="Distribution of WAP intensity excluding null readings")
```
As expected we now have a good looking normal distribution. It is slightly skewed to the right.


```{r - Plot of the not related to Wireless Access Points power}
hist(df_train$LONGITUDE,xlab="Longitude",main="Distribution of Longitudes")
hist(df_train$LATITUDE,xlab="Latitude",main="Distribution of Latitude")
hist(df_train$FLOOR,xlab="Floor",main="Distribution of Floors")
hist(df_train$BUILDINGID,xlab="BuildingID",main="Distribution of BuildingIDs")
hist(df_train$SPACEID,xlab="SpaceIDs",main="Distribution of SpaceIDs")
hist(df_train$RELATIVEPOSITION,xlab="Relative Position",main="Distribution of Relative Positions")

hist(df_train$USERID,xlab="User ID",main="Distribution of UserIDs")
hist(df_train$PHONEID,xlab="Phone ID",main="Distribution of Phone IDs")
```


Let's confirm that the distributions are similar in the validation dataset:

```{r -  Plot WAP variables in a single graph without null readings - Validation}
hist((stack(df_validation %>% select(starts_with("WAP"))) %>% filter(values <100))$values,xlab="WAP power",main="Distribution of WAP intensity excluding null readings")
```

```{r - Plot of the not related to Wireless Access Points power - Validation}
hist(df_validation$LONGITUDE,xlab="Longitude",main="Distribution of Longitudes")
hist(df_validation$LATITUDE,xlab="Latitude",main="Distribution of Latitude")
hist(df_validation$FLOOR,xlab="Floor",main="Distribution of Floors")
hist(df_validation$BUILDINGID,xlab="BuildingID",main="Distribution of BuildingIDs")
hist(df_validation$SPACEID,xlab="SpaceIDs",main="Distribution of SpaceIDs")
hist(df_validation$RELATIVEPOSITION,xlab="Relative Position",main="Distribution of Relative Positions")

hist(df_validation$USERID,xlab="User ID",main="Distribution of UserIDs")
hist(df_validation$PHONEID,xlab="Phone ID",main="Distribution of Phone IDs")
```



The distributions are quite similar, the exceptions are in the SPACEID, RELATIVEPOSITION and USERID where the validation dataset is constant throughout. We will confirm this below:

```{r - Non-variant columns in validation df}
table(df_validation$SPACEID)
table(df_validation$RELATIVEPOSITION)
table(df_validation$USERID)
```
Ye, as expected all are null.




*2nd round of Preprocessing the data:*

**Remove variables without any variation**
I noticed that many variables haven't got any variation. It is useless to incorporate these features in any model as they won't add any new information. 
Therefore, I will identify and later remove these variables.

```{r - Identify and remove constant variables}
constant_variables <- names(df_train[, sapply(df_train, function(v) var(v, na.rm=TRUE)==0)])
df_train <- df_train[,sapply(df_train, function(v) var(v, na.rm=TRUE)!=0)]
constant_variables
```

**Sampling**
As mentioned in the Plan of attack, the dataset is too large. Therefore, I will utilize a random subset of the data for the machine learning portion of this project.
```{r - Gather random sample of 2500 entries from train dataset:}
set.seed(42) #Set seed to ensure reproducibility
df_sample_train <- df_train[sample(nrow(df_train), 2500),]
```

```{r - Identify and remove constant variables in sample dataset}
constant_variables <- names(df_sample_train[, sapply(df_sample_train, function(v) var(v, na.rm=TRUE)==0)])
df_sample_train <- df_sample_train[,sapply(df_sample_train, function(v) var(v, na.rm=TRUE)!=0)]
constant_variables
```

*ML Modeling*

Felice recomended to forecast in cascade, firstly classify the easiest parameters (such as building; floor; inside or out...) and then make a regression for location (longitude and latitude).

Let's do that.


The easiest parameter seems to be the relative position (in or out), then the Building ID and finally the floor.
Having these 3 parameters it will hopefully be easy to infer SpaceID through a last classification model.

Experimentally, I found out that the buildingID is the easiest feature to forecast. This seems plausible because this ought to be the feature with less overlap of WAP signaling: a change in the building value imposes the largest physical distance movimentation relatively to a floor or going outside. In other words, a given WAP can send signal to multiple floors or both inside and outside, but due to increased distance, it seems unlikely that it is capable of sending signals to multiple buildings. Additionaly, knowing the building is helpful when forecasting other variables as can be seen here:

```{r - Plot buildingID against floor and relativeposition.}
Building_Floor_graph<-ggplot(data=df_train, aes(x=BUILDINGID, fill = factor(FLOOR))) + geom_bar()
Building_Floor_graph

Building_InsOut_graph<-ggplot(data=df_train, aes(x=BUILDINGID, fill = factor(RELATIVEPOSITION))) + geom_bar()
Building_InsOut_graph

```


**Define Models**

I chose the following models because these are the ones I am most familiarized and from what I understood, provide good results right out of the bat:
Support Vector Machine;
Gradient Boosting;
Random Forest;
```{r - 10 fold cross validation for 3 models to experiment}
svmfitControl <- trainControl(method = "repeatedcv", 
                              number = 3, # number of folds
                              repeats = 1) #the number of complete sets of folds to compute
gbmfitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
rffitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
```


**Building ID**

First of all let's define the usable features and the target for this model:
```{r}
df_train_BID <- df_sample_train %>% select(starts_with("WAP"),"BUILDINGID")
#Perform factorization on target column:
df_train_BID$BUILDINGID <- as.factor(as.character(df_train_BID$BUILDINGID)) #Reconvert brand into binary column not integer.
```

Because this dataset is slightly unbalanced, I will attempt to train according to the kappa metric instead of accuracy.
(Practically, Cohen’s kappa removes the possibility of the classifier and a random guess agreeing and measures the number of predictions it makes that cannot be explained by a random guess. Furthermore, Cohen’s kappa tries to correct the evaluation bias by taking into account the correct classification by a random guess.)


Let's train each of the 3 models with the training dataset:
## SVM:
```{r - Train Support Vector Machine Classification model}
svmfit_BID <- train(BUILDINGID~., #y/target
                 data = df_train_BID, #X/features
                 method = "svmLinear", #ML algorithm
                 trControl=svmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
                )

svmfit_BID
```
## GBM:
```{r - Train Gradient Boost Machine Classification model}
gbmfit_BID <- train(BUILDINGID~., #y/target
                 data = df_train_BID, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

gbmfit_BID
```
## RF:
```{r - Train Random Forest Classification model}
rffit_BID <- train(BUILDINGID~., #y/target
                 data = df_train_BID, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

rffit_BID
```

As asked, in this part we are supposed to train 3 models, which is what is coded above.

Train function automatically picks the iteration with the largest performance value (the one with best Kappa in this case), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in each model training}
varImp(svmfit_BID)

varImp(gbmfit_BID)

varImp(rffit_BID)
```


A few WAPs are recurrent, such as 087 (top 1 for both SVM and GBM and top 8 for RF), 078 (top 3 for SVM, top 7 for GBM and top 1 for RF)...
This means that we do not need to use all the 500 WAPs. A portion of them will be more than enough.


As predicted the salary variable is the most relevant in predicting the target variable. Age comes in a close second place. The remaining variables have a really low relative impact when forecasting the target.

```{r - Make predictions based on GBM trained model and calculate basic metrics (with PostResample)}
BID_pred_SVM <- predict(svmfit_BID, newdata = d_train)
confusionMatrix(data = BID_pred_SVM, 
                reference = factor(df_train$BUILDINGID))

BID_pred_GBM <- predict(gbmfit_BID, newdata = df_train)
confusionMatrix(data = BID_pred_GBM, 
                reference = factor(df_train$BUILDINGID))

BID_pred_RF <- predict(rffit_BID, newdata = df_train)
confusionMatrix(data = BID_pred_RF, 
                reference = factor(df_train$BUILDINGID))
```


This model provides an accuracy of ~94% which is pretty high.
The Kappa or Cohen's Kappa is at ~86% which is quite impressive since this metric is normalized at the baseline of random chance on our dataset.
This dataset has a slightly unbalanced target, therefore the Kappa value is quite different from the accuracy value.
It's worth adding that if the model was trained to optimise the accuracy, the metrics accuracy and Kappa obtained would be respectively: 0.932498 0.856289. Therefore optimising this model to the metric "Kappa" is a much better alternative for this dataset as it provided better results in both metrics!




Compare the results from the three models:


```{r - Compare BUILDINGID results:}
results <- resamples(list(LB_Model=svmfit_BID, GBM_Model=gbmfit_BID, RF_Model=rffit_BID))
summary(results)
bwplot(results)
```
Even though it is pretty close, it seems that the best model is the Logistic Boosting. Therefore we will use the predicted outcome from this model.
But first I will recreate the model with less features to make it more robust:

```{r - Recreate the best model:}

col_index <- varImp(gbmfit_BID)$importance %>% 
  mutate(names=row.names(.)) %>%
  arrange(-Overall)

imp_names <- col_index$names[1:100]

df_train_BID_final <- cbind(df_train_BID[,imp_names],
                            BUILDINGID = df_train_BID$BUILDINGID)

final_fit_BID <- train(BUILDINGID~., #y/target
                 data = df_train_BID_final, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

results <- resamples(list(LB_Model=svmfit_BID, GBM_Model=gbmfit_BID, RF_Model=rffit_BID, BestModel = final_fit_BID))
summary(results)
bwplot(results)

```


```{r - Create prediction for validation dataset with outcome from best model:}

Validation_BID_pred <- predict(gbmfit_BID, newdata = df_validation)
Validation_BID_pred

```

```{r - Bind WAP values with predicted values to create dataset for next level of cascade}
Step1_df_validation <- cbind(df_validation %>% select(starts_with("WAP")), BUILDINGID_pred =Validation_BID_pred)
Step1_df_validation
```


**Relative Position**

First of all let's define the usable features and the target for this model:
```{r}
df_train_RP <- df_sample_train %>% select(starts_with("WAP"),"RELATIVEPOSITION")
#Perform factorization on target column:
df_train_RP$RELATIVEPOSITION <- as.factor(as.character(df_train_RP$RELATIVEPOSITION)) #Reconvert brand into binary column not integer.
```

Because this dataset is slightly unbalanced, I will attempt to train according to the kappa metric instead of accuracy.
(Practically, Cohen’s kappa removes the possibility of the classifier and a random guess agreeing and measures the number of predictions it makes that cannot be explained by a random guess. Furthermore, Cohen’s kappa tries to correct the evaluation bias by taking into account the correct classification by a random guess.)



Let's train each of the 3 models with the training dataset:

```{r - Train Naive Bayes Classification model}
nbfit_RP <- train(RELATIVEPOSITION~., #y/target
                 data = df_train_RP, #X/features
                 method = "nb", #ML algorithm
                 trControl=nbfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
                )

nbfit_RP
```

```{r - Train Gradient Boost Machine Classification model}
gbmfit_RP <- train(RELATIVEPOSITION~., #y/target
                 data = df_train_RP, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

gbmfit_RP
```

```{r - Train Random Forest Classification model}
rffit_RP <- train(RELATIVEPOSITION~., #y/target
                 data = df_train_RP, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)



rffit_RP
```

As asked, in this part we are supposed to train a model using Stochastic Gradient Boosting, GBM, on the training set with 10-fold cross-validation and an Automatic Tuning Grid, which is what is coded above.

Train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in each model training}
varImp(nbfit_RP)

varImp(gbmfit_RP)

varImp(rffit_RP)
```


Note, the validation_df has a meaningless value for RELATIVEPOSITON, therefore I will calculate the confusion matrix for the training dataset:
```{r - Make predictions based on GBM trained model and calculate confusion matrix}
pred_GBM_RP <- predict(gbmfit_RP, newdata = df_train_RP)

confusionMatrix(data = pred_GBM_RP, 
                reference = df_train_RP$RELATIVEPOSITION, 
                positive = "1")
```

This model provides an accuracy of ~94% which is pretty high.
The Kappa or Cohen's Kappa is at ~86% which is quite impressive since this metric is normalized at the baseline of random chance on our dataset.
This dataset has a slightly unbalanced target, therefore the Kappa value is quite different from the accuracy value.
It's worth adding that if the model was trained to optimise the accuracy, the metrics accuracy and Kappa obtained would be respectively: 0.932498 0.856289. Therefore optimising this model to the metric "Kappa" is a much better alternative for this dataset as it provided better results in both metrics!


```{r - Further metrics with confusion matrix}
confusionMatrix(data = pred_GBM, 
                reference = testing$brand, 
                positive = "1")
```


```{r - Calculate two class summary for binary GBM model}
temp_pred_GB = factor(ifelse(pred_GBM == 1, "Y", "N"))
temp_obs_GB = factor(ifelse(testing$brand == 1, "Y", "N"))

twoClassSummary(data = data.frame(obs = temp_obs_GB, pred = temp_pred_GB, Y = Prob_pred_GBM$'1', N = Prob_pred_GBM$'0'), lev = levels(temp_pred_GB))
#For some reason, this function does not seem to provide ROC results if a binary numerical labeling is used, I had to convert '1' into 'Y' and '0' into 'N' to obtain valid results

mnLogLoss(data = data.frame(obs = temp_obs_GB, pred = temp_pred_GB, Y = Prob_pred_GBM$'1', N = Prob_pred_GBM$'0'), lev = levels(temp_pred_GB))
```



*Compare the results from two models:*
```{r - Compare results}
results <- resamples(list(GBMAut=gbmFit1, RFMan=rfFit1))

summary(results)

bwplot(results)
```




**Floor**

First of all let's define the usable features and the target for this model:
```{r}
df_train_F <- df_sample_train %>% select(starts_with("WAP"),"FLOOR")
#Perform factorization on target column:
df_train_F$FLOOR <- as.factor(as.character(df_train_F$FLOOR)) #Reconvert brand into binary column not integer.
```

Because this dataset is slightly unbalanced, I will attempt to train according to the kappa metric instead of accuracy.
(Practically, Cohen’s kappa removes the possibility of the classifier and a random guess agreeing and measures the number of predictions it makes that cannot be explained by a random guess. Furthermore, Cohen’s kappa tries to correct the evaluation bias by taking into account the correct classification by a random guess.)



Let's train each of the 3 models with the training dataset:

```{r - Train Naive Bayes Classification model}
nbfit_F <- train(FLOOR~., #y/target
                 data = df_train_F, #X/features
                 method = "nb", #ML algorithm
                 trControl=nbfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
                )

nbfit_F
```

```{r - Train Gradient Boost Machine Classification model}
gbmfit_F <- train(FLOOR~., #y/target
                 data = df_train_F, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

gbmfit_F
```

```{r - Train Random Forest Classification model}
rffit_RP <- train(FLOOR~., #y/target
                 data = df_train_F, #X/features
                 metric = 'Kappa', #Metric applied
                 method = "rf", #ML algorithm
                 trControl=rffitControl, #Apply CV to the training
                 tuneLength = 2, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)



rffit_F
```

As asked, in this part we are supposed to train a model using Stochastic Gradient Boosting, GBM, on the training set with 10-fold cross-validation and an Automatic Tuning Grid, which is what is coded above.

Train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in each model training}
varImp(nbfit_F)

varImp(gbmfit_F)

varImp(rffit_F)
```




As predicted the salary variable is the most relevant in predicting the target variable. Age comes in a close second place. The remaining variables have a really low relative impact when forecasting the target.

```{r - Make predictions based on GBM trained model and calculate basic metrics (with PostResample)}
pred_GBM <- predict(gbmFit1, newdata = testing)

Prob_pred_GBM <- predict(gbmFit1, newdata = testing, type = "prob")

postResample(pred_GBM, testing$brand)
```

This model provides an accuracy of ~94% which is pretty high.
The Kappa or Cohen's Kappa is at ~86% which is quite impressive since this metric is normalized at the baseline of random chance on our dataset.
This dataset has a slightly unbalanced target, therefore the Kappa value is quite different from the accuracy value.
It's worth adding that if the model was trained to optimise the accuracy, the metrics accuracy and Kappa obtained would be respectively: 0.932498 0.856289. Therefore optimising this model to the metric "Kappa" is a much better alternative for this dataset as it provided better results in both metrics!


```{r - Further metrics with confusion matrix}
confusionMatrix(data = pred_GBM, 
                reference = testing$brand, 
                positive = "1")
```






