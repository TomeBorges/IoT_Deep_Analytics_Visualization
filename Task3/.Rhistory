trControl=gbmfitControl, #Apply CV to the training
tuneLength = 10, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
df_train_RP <- df_sample_train %>% select(starts_with("WAP"),"RELATIVEPOSITION")
#Perform factorization on target column:
df_train_RP$RELATIVEPOSITION <- as.factor(df_train_RP$RELATIVEPOSITION) #Reconvert brand into binary column not integer.
nbfitControl <- trainControl(method = "repeatedcv",
number = 3, # number of folds
repeats = 1) #the number of complete sets of folds to compute
gbmfitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
rffitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
nbfit_RP <- train(RELATIVEPOSITION~., #y/target
data = df_train_RP, #X/features
method = "nbDiscrete", #ML algorithm
trControl=nbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
df_train_RP <- df_sample_train %>% select(starts_with("WAP"),"RELATIVEPOSITION")
#Perform factorization on target column:
df_train_RP$RELATIVEPOSITION <- as.factor(as.character(df_train_RP$RELATIVEPOSITION)) #Reconvert brand into binary column not integer.
nbfitControl <- trainControl(method = "repeatedcv",
number = 3, # number of folds
repeats = 1) #the number of complete sets of folds to compute
gbmfitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
rffitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
nbfit_RP <- train(RELATIVEPOSITION~., #y/target
data = df_train_RP, #X/features
method = "nbDiscrete", #ML algorithm
trControl=nbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
df_train_RP%RELATIVEPOSITION$RELATIVEPOSITION
df_train_RP$RELATIVEPOSITION
nbfitControl <- trainControl(method = "repeatedcv",
number = 3, # number of folds
repeats = 1) #the number of complete sets of folds to compute
gbmfitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
rffitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
nbfit_RP <- train(RELATIVEPOSITION~., #y/target
data = df_train_RP, #X/features
method = "nbDiscrete", #ML algorithm
trControl=nbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
table(sapply(df_train_RP, class))
sapply(df_train_RP, class)
nbfit_RP <- train(RELATIVEPOSITION~., #y/target
data = df_train_RP, #X/features
method = "nb", #ML algorithm
trControl=nbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
nbfit_RP
gbmfit_RP <- train(RELATIVEPOSITION~., #y/target
data = df_train_RP, #X/features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 10, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
gbmfit_RP <- train(RELATIVEPOSITION~., #y/target
data = df_train_RP, #X/features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
gbmfit_RP
rffit_RP <- train(RELATIVEPOSITION~., #y/target
data = df_train_RP, #X/features
metric = 'Kappa', #Metric applied
method = "rf", #ML algorithm
trControl=rffitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
varImp(nbfit_RP)
varImp(gbmfit_RP)
varImp(rffit_RP)
pred_GBM <- predict(gbmFit1, newdata = testing)
pred_GBM_RP <- predict(gbmfit_RP, newdata = testing)
postResample(pred_GBM_RP, df_validation$RELATIVEPOSITION)
pred_GBM_RP <- predict(gbmfit_RP, newdata = df_validation)
postResample(pred_GBM_RP, df_validation$RELATIVEPOSITION)
pred_GBM_RP
confusionMatrix(data = pred_GBM_RP,
reference = df_train_RP$RELATIVEPOSITION,
positive = "1")
pred_GBM_RP <- predict(gbmfit_RP, newdata = df_train_RP)
confusionMatrix(data = pred_GBM_RP,
reference = df_train_RP$RELATIVEPOSITION,
positive = "1")
pred_GBM_RP <- predict(gbmfit_RP, newdata = df_validation)
confusionMatrix(data = pred_GBM_RP,
reference = df_validation$RELATIVEPOSITION,
positive = "1")
pred_GBM_RP <- predict(gbmfit_RP, newdata = df_validation)
confusionMatrix(data = pred_GBM_RP,
reference = df_validation$RELATIVEPOSITION,
positive = "1")
pred_GBM_RP
df_validation
pred_GBM_RP
df_validation
df_validation$RELATIVEPOSITION
confusionMatrix(data = pred_GBM_RP,
reference = df_validation$RELATIVEPOSITION,
positive = "1")
confusionMatrix(data = pred_GBM_RP,
reference = df_train_RP$RELATIVEPOSITION,
positive = "1")
pred_GBM_RP <- predict(gbmfit_RP, newdata = df_train_RP)
pred_GBM_RP <- predict(gbmfit_RP, newdata = df_train_RP)
confusionMatrix(data = pred_GBM_RP,
reference = df_train_RP$RELATIVEPOSITION,
positive = "1")
df_train_BID <- df_sample_train %>% select(starts_with("WAP"),"BUILDINGID")
#Perform factorization on target column:
df_train_BID$BUILDINGID <- as.factor(as.character(df_train_BID$BUILDINGID)) #Reconvert brand into binary column not integer.
nbfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
method = "nb", #ML algorithm
trControl=nbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
nbfit_BID
df_train_F <- df_sample_train %>% select(starts_with("WAP"),"FLOOR")
#Perform factorization on target column:
df_train_F$FLOOR <- as.factor(as.character(df_train_F$FLOOR)) #Reconvert brand into binary column not integer.
nbfit_F <- train(FLOOR~., #y/target
data = df_train_F, #X/features
method = "nb", #ML algorithm
trControl=nbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
nbfit_F
gbmfit_F <- train(FLOOR~., #y/target
data = df_train_F, #X/features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
gbmfit_F
gbmfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
gbmfit_BID
nbfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
method = "logicBag", #ML algorithm
trControl=nbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
install.packages("logicFS")
nbfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
method = "LogitBoost", #ML algorithm
trControl=nbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
nbfit_BID
df_train_BID <- df_sample_train %>% select(starts_with("WAP"),"BUILDINGID")
#Perform factorization on target column:
df_train_BID$BUILDINGID <- as.factor(as.character(df_train_BID$BUILDINGID)) #Reconvert brand into binary column not integer.
nbfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
method = "LogitBoost", #ML algorithm
trControl=nbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
nbfit_BID
gbmfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
gbmfit_BID
rffit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
metric = 'Kappa', #Metric applied
method = "rf", #ML algorithm
trControl=rffitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
rffit_BID
nbfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
method = "LogitBoost", #ML algorithm
trControl=nbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
nbfit_BID
library(RMySQL)
library(caret)
library(klaR)
#Avoid loading the MASS package after dplyr
#It may cause a mess with the select function
library(dplyr)
library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(ggfortify)
library(forecast)
library(lubridate)
library(plyr)
library(plotly)
library(gbm)
library(Rcpp)
df_train<-read.csv('./UJIndoorLoc/trainingData.csv')
df_train
df_validation<-read.csv('./UJIndoorLoc/validationData.csv')
df_validation
length(names(df_train %>% select(starts_with("WAP"))))
names(df_train %>% select(!starts_with("WAP")))
length(names(df_validation %>% select(starts_with("WAP"))))
names(df_validation %>% select(!starts_with("WAP")))
table(sapply(df_train, class))
table(sapply(df_validation, class))
df_train$DateTime <- as.POSIXct((df_train$TIMESTAMP), origin="1970-01-01") #Default origin time
df_validation$DateTime <- as.POSIXct((df_validation$TIMESTAMP), origin="1970-01-01") #Default origin time
any(is.na(df_train))
any(is.na(df_validation))
hist(stack(df_train %>% select(starts_with("WAP")))$values,xlab="WAP power",main="Distribution of WAP intensity")
hist((stack(df_train %>% select(starts_with("WAP"))) %>% filter(values <100))$values,xlab="WAP power",main="Distribution of WAP intensity excluding null readings")
hist(df_train$LONGITUDE,xlab="Longitude",main="Distribution of Longitudes")
hist(df_train$LATITUDE,xlab="Latitude",main="Distribution of Latitude")
hist(df_train$FLOOR,xlab="Floor",main="Distribution of Floors")
hist(df_train$BUILDINGID,xlab="BuildingID",main="Distribution of BuildingIDs")
hist(df_train$SPACEID,xlab="SpaceIDs",main="Distribution of SpaceIDs")
hist(df_train$RELATIVEPOSITION,xlab="Relative Position",main="Distribution of Relative Positions")
hist(df_train$USERID,xlab="User ID",main="Distribution of UserIDs")
hist(df_train$PHONEID,xlab="Phone ID",main="Distribution of Phone IDs")
hist((stack(df_validation %>% select(starts_with("WAP"))) %>% filter(values <100))$values,xlab="WAP power",main="Distribution of WAP intensity excluding null readings")
hist(df_validation$LONGITUDE,xlab="Longitude",main="Distribution of Longitudes")
hist(df_validation$LATITUDE,xlab="Latitude",main="Distribution of Latitude")
hist(df_validation$FLOOR,xlab="Floor",main="Distribution of Floors")
hist(df_validation$BUILDINGID,xlab="BuildingID",main="Distribution of BuildingIDs")
hist(df_validation$SPACEID,xlab="SpaceIDs",main="Distribution of SpaceIDs")
hist(df_validation$RELATIVEPOSITION,xlab="Relative Position",main="Distribution of Relative Positions")
hist(df_validation$USERID,xlab="User ID",main="Distribution of UserIDs")
hist(df_validation$PHONEID,xlab="Phone ID",main="Distribution of Phone IDs")
table(df_validation$SPACEID)
table(df_validation$RELATIVEPOSITION)
table(df_validation$USERID)
constant_variables <- names(df_train[, sapply(df_train, function(v) var(v, na.rm=TRUE)==0)])
df_train <- df_train[,sapply(df_train, function(v) var(v, na.rm=TRUE)!=0)]
constant_variables
set.seed(42) #Set seed to ensure reproducibility
df_sample_train <- df_train[sample(nrow(df_train), 2500),]
constant_variables <- names(df_sample_train[, sapply(df_sample_train, function(v) var(v, na.rm=TRUE)==0)])
df_sample_train <- df_sample_train[,sapply(df_sample_train, function(v) var(v, na.rm=TRUE)!=0)]
constant_variables
lbfitControl <- trainControl(method = "repeatedcv",
number = 3, # number of folds
repeats = 1) #the number of complete sets of folds to compute
gbmfitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
rffitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
df_train_BID <- df_sample_train %>% select(starts_with("WAP"),"BUILDINGID")
#Perform factorization on target column:
df_train_BID$BUILDINGID <- as.factor(as.character(df_train_BID$BUILDINGID)) #Reconvert brand into binary column not integer.
lbfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
method = "LogitBoost", #ML algorithm
trControl=lbfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
lbfit_BID
gbmfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
gbmfit_BID
ggplot(data=train_df, aes(x=FLOOR,fill = BUILDINGID)) + geom_histogram()
ggplot(data=df_train, aes(x=FLOOR,fill = BUILDINGID)) + geom_histogram()
ggplot(data=df_train, aes(x=df_train$FLOOR, fill = BUILDINGID)) + geom_histogram()
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram()
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram()
df_train
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram()
library(ggplot2)
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram()
iris
ggplot(data=df_train, aes(x=as.factor(FLOOR), fill = as.factor(BUILDINGID))) + geom_histogram()
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram()
df_train[c(FLOOR; BUILDINGID)]
df_train[c(FLOOR, BUILDINGID)]
df_validation[c(FLOOR, BUILDINGID)]
df_validation[FLOOR, BUILDINGID]
df_validation[[FLOOR, BUILDINGID]]
df_validation[c(FLOOR, BUILDINGID)]
ggplot(aes(x=df_train$FLOOR, fill = df_train$BUILDINGID)) + geom_histogram()
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram()
#plot the entire data set (everything)
hist(df_train$FLOOR[BUILDINGID==0], breaks=c(1:10), col="Red")
#plot the entire data set (everything)
hist(df_train$FLOOR[df_train$BUILDINGID==0], breaks=c(1:10), col="Red")
#plot the entire data set (everything)
hist(df_train$FLOOR[df_train$BUILDINGID==0] col="Red")
#plot the entire data set (everything)
hist(df_train$FLOOR[df_train$BUILDINGID==0], col="Red")
hist(df_train$FLOOR[df_train$BUILDINGID==1], col="Blue", add=TRUE)
#plot the entire data set (everything)
hist(df_train$FLOOR[df_train$BUILDINGID==0], col="Red")
hist(df_train$FLOOR[df_train$BUILDINGID==1], col="Blue", add=TRUE)
hist(df_train$FLOOR[df_train$BUILDINGID==2], col="Green", add=TRUE)
#plot the entire data set (everything)
hist(df_train$FLOOR[df_train$BUILDINGID==0], col="Red")
hist(df_train$FLOOR[df_train$BUILDINGID==1], col="Blue", add=TRUE)
hist(df_train$FLOOR[df_train$BUILDINGID==2], col="Green", add=TRUE)
#plot the entire data set (everything)
hist(df_train$FLOOR[df_train$BUILDINGID==0], col="Red")
hist(df_train$FLOOR[df_train$BUILDINGID==1], col="Blue", add=TRUE)
hist(df_train$FLOOR[df_train$BUILDINGID==2], col="Green", add=TRUE)
df_train
library(ggplot2)
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram()
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID), binwidth = 5) + geom_histogram()
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram(binwidth =5)
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram(binwidth = 4)
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram(binwidth = 40)
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram(binwidth = 1)
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram(binwidth = 0)
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram(binwidth = 1)
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram(binwidth = 2)
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram()
ggplot(data=df_train, aes(FLOOR, fill = BUILDINGID)) + geom_histogram()
ggplot(df_train, aes(factor(FLOOR), fill = BUILDINGID)) + geom_histogram()
ggplot(df_train, aes(factor(FLOOR), fill = factor(BUILDINGID))) + geom_histogram()
ggplot(df_train, aes(factor(FLOOR), fill = factor(BUILDINGID)))
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID)) + geom_histogram()
ggplot(data=df_train, aes(x=FLOOR, fill = BUILDINGID))
df_train$BUILDINGID
df_train$FLOOR
ggplot(mtcars, aes(factor(cyl), fill = factor(vs)))
svmfit_LONG <- train(LONGITUDE~., #y/target
data = df_train_LONG, #X/features
method = "svmLinear", #ML algorithm
trControl=svmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
library(RMySQL)
library(caret)
library(klaR)
#Avoid loading the MASS package after dplyr
#It may cause a mess with the select function
library(dplyr)
library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(ggfortify)
library(forecast)
library(lubridate)
library(plyr)
library(plotly)
library(gbm)
library(Rcpp)
df_train<-read.csv('./UJIndoorLoc/trainingData.csv')
df_train
df_validation<-read.csv('./UJIndoorLoc/validationData.csv')
df_validation
length(names(df_train %>% select(starts_with("WAP"))))
names(df_train %>% select(!starts_with("WAP")))
length(names(df_validation %>% select(starts_with("WAP"))))
names(df_validation %>% select(!starts_with("WAP")))
table(sapply(df_train, class))
table(sapply(df_validation, class))
df_train$DateTime <- as.POSIXct((df_train$TIMESTAMP), origin="1970-01-01") #Default origin time
df_validation$DateTime <- as.POSIXct((df_validation$TIMESTAMP), origin="1970-01-01") #Default origin time
any(is.na(df_train))
any(is.na(df_validation))
hist(stack(df_train %>% select(starts_with("WAP")))$values,xlab="WAP power",main="Distribution of WAP intensity")
hist((stack(df_train %>% select(starts_with("WAP"))) %>% filter(values <100))$values,xlab="WAP power",main="Distribution of WAP intensity excluding null readings")
hist(df_train$LONGITUDE,xlab="Longitude",main="Distribution of Longitudes")
hist(df_train$LATITUDE,xlab="Latitude",main="Distribution of Latitude")
hist(df_train$FLOOR,xlab="Floor",main="Distribution of Floors")
hist(df_train$BUILDINGID,xlab="BuildingID",main="Distribution of BuildingIDs")
hist(df_train$SPACEID,xlab="SpaceIDs",main="Distribution of SpaceIDs")
hist(df_train$RELATIVEPOSITION,xlab="Relative Position",main="Distribution of Relative Positions")
hist(df_train$USERID,xlab="User ID",main="Distribution of UserIDs")
hist(df_train$PHONEID,xlab="Phone ID",main="Distribution of Phone IDs")
hist((stack(df_validation %>% select(starts_with("WAP"))) %>% filter(values <100))$values,xlab="WAP power",main="Distribution of WAP intensity excluding null readings")
hist(df_validation$LONGITUDE,xlab="Longitude",main="Distribution of Longitudes")
hist(df_validation$LATITUDE,xlab="Latitude",main="Distribution of Latitude")
hist(df_validation$FLOOR,xlab="Floor",main="Distribution of Floors")
hist(df_validation$BUILDINGID,xlab="BuildingID",main="Distribution of BuildingIDs")
hist(df_validation$SPACEID,xlab="SpaceIDs",main="Distribution of SpaceIDs")
hist(df_validation$RELATIVEPOSITION,xlab="Relative Position",main="Distribution of Relative Positions")
hist(df_validation$USERID,xlab="User ID",main="Distribution of UserIDs")
hist(df_validation$PHONEID,xlab="Phone ID",main="Distribution of Phone IDs")
table(df_validation$SPACEID)
table(df_validation$RELATIVEPOSITION)
table(df_validation$USERID)
constant_variables <- names(df_train[, sapply(df_train, function(v) var(v, na.rm=TRUE)==0)])
df_train <- df_train[,sapply(df_train, function(v) var(v, na.rm=TRUE)!=0)]
constant_variables
set.seed(42) #Set seed to ensure reproducibility
df_sample_train <- df_train[sample(nrow(df_train), 2500),]
constant_variables <- names(df_sample_train[, sapply(df_sample_train, function(v) var(v, na.rm=TRUE)==0)])
df_sample_train <- df_sample_train[,sapply(df_sample_train, function(v) var(v, na.rm=TRUE)!=0)]
constant_variables
Building_Floor_graph<-ggplot(data=df_train, aes(x=BUILDINGID, fill = factor(FLOOR))) + geom_bar()
Building_Floor_graph
Building_InsOut_graph<-ggplot(data=df_train, aes(x=BUILDINGID, fill = factor(RELATIVEPOSITION))) + geom_bar()
Building_InsOut_graph
Building_InsOut_graph<-ggplot(data=df_train, aes(x=FLOOR, fill = factor(RELATIVEPOSITION))) + geom_bar()
Building_InsOut_graph
svmfitControl <- trainControl(method = "repeatedcv",
number = 3, # number of folds
repeats = 1) #the number of complete sets of folds to compute
gbmfitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
rffitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
df_train_BID <- df_sample_train %>% select(starts_with("WAP"),"BUILDINGID")
#Perform factorization on target column:
df_train_BID$BUILDINGID <- as.factor(as.character(df_train_BID$BUILDINGID)) #Reconvert feature into binary column not integer.
svmfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
method = "svmLinear", #ML algorithm
trControl=svmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
svmfit_BID
gbmfit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
gbmfit_BID
rffit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID, #X/features
metric = 'Kappa', #Metric applied
method = "rf", #ML algorithm
trControl=rffitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
rffit_BID
varImp(svmfit_BID)
varImp(gbmfit_BID)
varImp(rffit_BID)
BID_pred_SVM <- predict(svmfit_BID, newdata = df_train)
confusionMatrix(data = BID_pred_SVM,
reference = factor(df_train$BUILDINGID))
BID_pred_GBM <- predict(gbmfit_BID, newdata = df_train)
confusionMatrix(data = BID_pred_GBM,
reference = factor(df_train$BUILDINGID))
BID_pred_RF <- predict(rffit_BID, newdata = df_train)
confusionMatrix(data = BID_pred_RF,
reference = factor(df_train$BUILDINGID))
results <- resamples(list(SVM_Model=svmfit_BID, GBM_Model=gbmfit_BID, RF_Model=rffit_BID))
summary(results)
bwplot(results)
col_index <- varImp(gbmfit_BID)$importance %>%
mutate(names=row.names(.)) %>%
arrange(-Overall)
imp_names <- col_index$names[1:250]
df_train_BID_final <- cbind(df_train_BID[,imp_names],
BUILDINGID = df_train_BID$BUILDINGID)
final_fit_BID <- train(BUILDINGID~., #y/target
data = df_train_BID_final, #X/features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
results <- resamples(list(SVM_Model=svmfit_BID, GBM_Model=gbmfit_BID, RF_Model=rffit_BID, BestModel = final_fit_BID))
summary(results)
bwplot(results)
BID_pred_final <- predict(final_fit_BID, newdata = df_train)
confusionMatrix(data = BID_pred_final,
reference = factor(df_train$BUILDINGID))
Validation_BID_pred <- predict(final_fit_BID, newdata = df_validation)
Validation_BID_pred
Step1_df_validation <- cbind(df_validation %>% select(starts_with("WAP")), BUILDINGID_pred =Validation_BID_pred)
Step1_df_validation
df_train_F <- df_sample_train %>% select(starts_with("WAP"),"BUILDINGID","FLOOR")
#Perform factorization on target column:
df_train_F$FLOOR <- as.factor(as.character(df_train_F$FLOOR)) #Reconvert brand feature binary column not integer.
#df_train_F$BUILDINGID <- as.factor(as.integer(df_train_F$BUILDINGID)) #Reconvert feature into binary column not integer.
svmfit_F <- train(FLOOR~., #y/target
data = df_train_F, #X/features
method = "svmLinear", #ML algorithm
trControl=svmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated.
)
svmfit_F
gbmfit_F <- train(FLOOR~., #y/target
data = df_train_F, #X/features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
gbmfit_F
rffit_F <- train(FLOOR~., #y/target
data = df_train_F, #X/features
metric = 'Kappa', #Metric applied
method = "rf", #ML algorithm
trControl=rffitControl, #Apply CV to the training
tuneLength = 2, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
