---
title: "Task 1: Domain Research and Exploratory Data Analysis"
output: html_notebook
---

This notebook contains the answers to Task 1: Domain Research and Exploratory Data Analysis.


```{r - Initial imports}
library(RMySQL)

library(caret)

library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(lubridate)
library(plyr)


library(gbm)
library(Rcpp)
```


*Retrieve data*


This archive contains 2075259 measurements (when retrieved through the SQL databse we have only 2049280 observations) gathered in a house located in Sceaux (7km of Paris, France) between December 2006 and November 2010 (47 months).
Notes:
1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.
2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.

**Retrieve data from a database:**
```{r - Create a database connection & list the tables contained in it}
con = dbConnect(MySQL(), user='deepAnalytics', password='Sqltask1234!', dbname='dataanalytics2018', host='data-analytics-2018.cbrosir2cswx.us-east-1.rds.amazonaws.com')

dbListTables(con)
```

```{r - Lists attributes contained in a table and confirm that all tables are in the same format}
dbListFields(con,'yr_2006')

(dbListFields(con,'yr_2006') == dbListFields(con,'yr_2007')) && (dbListFields(con,'yr_2006') == dbListFields(con,'yr_2008')) && (dbListFields(con,'yr_2006') == dbListFields(con,'yr_2008')) && (dbListFields(con,'yr_2006') == dbListFields(con,'yr_2009')) && (dbListFields(con,'yr_2006') == dbListFields(con,'yr_2010'))
```

```{r - Query databse to download data of all available years}
## Use asterisk to specify all attributes for download
yr_2006 <- dbGetQuery(con, "SELECT Date, Time, Sub_metering_1, Sub_metering_2, Sub_metering_3 FROM yr_2006")
yr_2007 <- dbGetQuery(con, "SELECT Date, Time, Sub_metering_1, Sub_metering_2, Sub_metering_3 FROM yr_2007")
yr_2008 <- dbGetQuery(con, "SELECT Date, Time, Sub_metering_1, Sub_metering_2, Sub_metering_3 FROM yr_2008")
yr_2009 <- dbGetQuery(con, "SELECT Date, Time, Sub_metering_1, Sub_metering_2, Sub_metering_3 FROM yr_2009")
yr_2010 <- dbGetQuery(con, "SELECT Date, Time, Sub_metering_1, Sub_metering_2, Sub_metering_3 FROM yr_2010")
```

We have queried each year individually. The datasets contain 21992, 521669, 526905, 521320 and 457394 entries respectively.
Now I will investigate each dataframe individually to verify that each covers a whole year:

```{r - Look into dataframe from 2006:}
str(yr_2006)
summary(yr_2006)
head(yr_2006[order(yr_2006$Date),]) #order applied in case data didn't come ordered from database
tail(yr_2006[order(yr_2006$Date),]) #order applied in case data didn't come ordered from database
```
```{r - Look into dataframe from 2007:}
str(yr_2007)
summary(yr_2007)
head(yr_2007[order(yr_2007$Date),]) #order applied in case data didn't come ordered from database
tail(yr_2007[order(yr_2007$Date),]) #order applied in case data didn't come ordered from database
```
```{r - Look into dataframe from 2008:}
str(yr_2008)
summary(yr_2008)
head(yr_2008[order(yr_2008$Date),]) #order applied in case data didn't come ordered from database
tail(yr_2008[order(yr_2008$Date),]) #order applied in case data didn't come ordered from database
```
```{r - Look into dataframe from 2009:}
str(yr_2009)
summary(yr_2009)
head(yr_2009[order(yr_2009$Date),]) #order applied in case data didn't come ordered from database
tail(yr_2009[order(yr_2009$Date),]) #order applied in case data didn't come ordered from database
```
```{r - Look into dataframe from 2010:}
str(yr_2010)
summary(yr_2010)
head(yr_2010[order(yr_2010$Date),]) #order applied in case data didn't come ordered from database
tail(yr_2010[order(yr_2010$Date),]) #order applied in case data didn't come ordered from database
```


All dataframes contain a whole year of data, except for the first and last. The first dataset starts at December 16 and the last one ends in November 26. Anyhow, we have got a continuous dataflow from 16th December 2006 until 26th November 2010.
Therefore lets join the individual dataframes into a single one to make it more manageable:

```{r - Combine tables into one dataframe using dplyr}
df <- bind_rows(yr_2006, yr_2007, yr_2008, yr_2009, yr_2010)
```

```{r - Confirm if dates are in the correct order:}
str(df)
summary(df)
head(df)
tail(df)
```

Note: In order to avoid downloading each individual yearly dataframe, which is a slow process (maybe due to my internet connection), I will save it into memory therefore, next time I work on this project again (after wiping the computer's RAM memory clean) I can simply load it from memory.

```{r - Save df into memory}
write.csv(df, "household_power_consumption.csv")
```


**Retrieve data from memory:**
```{r - Read the dataset from memory}
df<-read.csv('household_power_consumption.csv')
df
```


*Preprocess the data:*

Since the Date and Time columns are separate they will need to be combined within the dataset in order to convert them to the correct format to complete the appropriate analysis:

```{r - Combine Date and Time attribute values in a new attribute column "DataTime"}
df <- cbind(df,DateTime = paste(df$Date,df$Time), stringsAsFactors=FALSE)
#Note: Alternatively I could edit the column name with the command below (if I downloaded more than 5 attributes I need to change the column number)
#colnames(yourdataframe)[6] <-"DateTime"

df
```

No need to keep the "Date" and "Time" individual columns, I will also remove these.
```{r - Move the DateTime attribute within the dataset and drop "Date" & "Time" columns}
#I could do this as suggested in the PoA
#df <- df[,c(ncol(df), 1:(ncol(df)-1))]
#head(df)
#But also, I can drop "Date" and "Time" columns therefore I will do these two things at once:
df <- df %>%
  select(X, DateTime, Sub_metering_1, Sub_metering_2, Sub_metering_3)
df
```

I will now want to convert the new DateTime attribute to a DateTime data type called POSIXct. After converting to POSIXct we will add the time zone to prevent warning messages. The data description suggests that the data is from France.

```{r - Convert DateTime from character to POSIXct }
## 
df$DateTime <- as.POSIXct(df$DateTime, "%Y/%m/%d %H:%M:%S")
```
The warning "unknown timezone '%Y/%m/%d %H:%M:%S'unknown timezone '%Y/%m/%d %H:%M:%S'" will be solved with the chunk below.


```{r - Add the time zone}
attr(df$DateTime, "tzone") <- "Europe/Paris"
```

```{r - Inspect the data types}
str(df)
```

Now, the DateTime is in the correct data type for its content, it is of POSIXct (POSIX calendar time)

Now, I will prepare Lubridate to extract DateTime information into individual attributes like Year and Month. These attributes can be used by dplyr's filter command to subset the data into useful data sets for visualization. The subsetting will be done in Task 2 of Course 3, but let's prepare the new attributes now. 
I will start with extracting "Year" information from DateTime using the Lubridate "year" function and create an attribute for year
```{r - Create "year" attribute with lubridate}
df$year <- year(df$DateTime)
df
```

```{r - Create additional time period separators for task 2?}
df$quarter <- quarter(df$DateTime)
df$month <- month(df$DateTime)
df$week <- week(df$DateTime)
#In order to show week days in English instead of Portuguese (my original locale in RStudio) I will run the following command:
Sys.setlocale("LC_TIME", "English")
df$weekdays <- weekdays(df$DateTime)
df$day <- day(df$DateTime)
df$hour <- hour(df$DateTime)
df$minute <- minute(df$DateTime)
df
```

*Initial Data exploration of the data:*

According to the documentation, here follows a brief overview of each column:
1.date: Date in format dd/mm/yyyy (now date part of DateTime column)
2.time: time in format hh:mm:ss (now date part of DateTime column)
..
7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the *kitchen*, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).
8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the *laundry room*, containing a washing-machine, a tumble-drier, a refrigerator and a light.
9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an *electric water-heater* and an *air-conditioner*.

Additionaly, according to the documentation, the dataset contains missing values in the measurements (nearly 1.25% of the rows), let's analyse this. In particular I will look into April 28, 2007 (which is the missing example the documentation mentioned):
```{r - Are there missing values?}
any(is.na(df))
```

```{r - Which columns have null values:}
names(which(colSums(is.na(df)) > 0))
```

It seems we haven't got any missing values according to R. Looking closely at the documentation, it is said that out of the original 2075259 rows, nearly 1.25% of the rows are missing. Therefore, we should have the following filled rows 2075259*(1-0.0125) = 2049318.2625, which is close to what was downloaded, therefore it seems that the downloaded version has been cleaned of missing values.
The last test is to look at April 28, 2007 and see how many rows do we have:
```{r}
subset(df, DateTime>= "2007-05-28" & DateTime < "2007-05-29")
```
As can be seen above, we have counted 1440 rows, each corresponds to a minute of the day which corresponds to 24 complete hours (1440 mins / 60 mins = 24 hours) or, putting it differently, to a whole day. I can conclude that we do not have the missings that the documentation was referring to.


```{r - Gather summary statistics}
summary(df)
```
Using the summary function, a few oconclusions can be taken. For the sub_metering groups, the minimum value does not contain any useful information, it is always null, which makes sense because no single house room is constantly consuming energy in a common household.
Now, going into further detail:
- Sub_metering_1, which correspondes to the kitchen, has got the lowest consumption of all groups. This group contains the lowest 3rd quantile and mean value. On the other hand, it contains the highest maximum value. A possibility for this fact could be due to when a person uses the kitchen to cook, typically more than one appliance is used and more intensively, the same is not necessarily true for the remaining sub_metering groups.
- Sub_metering_2, which corresponds to the laundry room, has got a higher 3rd quartile and mean than the first group. This suggests that the set of appliances measured in this group is in usage for longer periods of times. The maximum value though, is slightly lower than the kitchen froup suggesting less intensive usage of this room.
- Lastly, the sub_metering_3 group, referring to the electric water-heater and air-conditioner has got a much higher mean, median and 3rd quartile. Inline with the second group, this fact suggests that the 3rd group requires a much more prolonged usage. However, the maximum value is the lowest of all groups suggesting that this last group seldom has any peak usage. This makes sense because this appliances are supposed to be used continuously without requiring large energetic efforts.

In order to improve the quality of this dataset and consequently the analysis, one could:
1. Add attributes 
2. Instead of separating the sub-metering groups per room, one could separate according per usage intensity. This way, such an analysis would reveal further information about each type of appliance instead of room usage. The first group could contain continuously used appliances, such as the AC, water heater or fridge. The second could contain appliances not so frequently used, but when used these take a long time running, such as the dishwasher, washing machine or tumble-drier. And finally sporadically used appliances such as the microwave or the lights. This alternative grouping seems to be as valid as the original one, but the conclusions taken would be slightly different.


sporadic and low usage such as 






